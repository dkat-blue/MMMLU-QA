{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "c2e9e366",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "env: CUDA_LAUNCH_BLOCKING=1\n",
            "env: TQDM_NOTEBOOK=0\n",
            "env: WANDB_PROJECT=gemma-nlp_lab2_mmmlu_ft_corrected\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Runtime flags ───────────────────────────────────────────────────────────\n",
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "%env TQDM_NOTEBOOK=0\n",
        "%env WANDB_PROJECT=gemma-nlp_lab2_mmmlu_ft_corrected"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "30914d73",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%  ── One-shot dependency install ───────────────\n",
        "!pip -q install datasets transformers accelerate bitsandbytes \\\n",
        "               sentencepiece wandb huggingface_hub peft \\\n",
        "               matplotlib seaborn pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "31269899",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/denis/kpi/iasa_nlp_labs/mmmlu_question_answering/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Imports ────────────────────────────────────────────────────────────────\n",
        "import os, re, warnings, json, math, random, time\n",
        "import torch, numpy as np, pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "from datasets            import load_dataset, DatasetDict, concatenate_datasets\n",
        "from transformers        import (AutoTokenizer, AutoModelForCausalLM, TrainingArguments,\n",
        "                                 Trainer, BitsAndBytesConfig, default_data_collator)\n",
        "from peft                import (LoraConfig, get_peft_model,\n",
        "                                 prepare_model_for_kbit_training)\n",
        "from sklearn.metrics     import accuracy_score\n",
        "from huggingface_hub     import login\n",
        "import wandb\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "7e4d8723",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdenis-katkalo\u001b[0m (\u001b[33mdenis-katkalo-kpi\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.11"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>./outputs/wandb/run-20250528_015916-inhq9y7f</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected/runs/inhq9y7f' target=\"_blank\">gemma-2b-mmmlu-de-fr-qlora-corrected-2epochs-lr1e-4</a></strong> to <a href='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected' target=\"_blank\">https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected/runs/inhq9y7f' target=\"_blank\">https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected/runs/inhq9y7f</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected/runs/inhq9y7f?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
            ],
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fbb3c6f6680>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%  ── Configuration & Weights & Biases ───────────────────────────────────────\n",
        "MODEL_NAME                     = \"google/gemma-2-2b\"\n",
        "DATASET_NAME                   = \"openai/MMMLU\"\n",
        "DATASET_CONFIGS                = [\"DE_DE\", \"FR_FR\"]\n",
        "\n",
        "MAX_PROMPT_TOKENS_FOR_FILTER   = 256      # drop questions with huge context\n",
        "MAX_SEQ_LENGTH                 = 300      # Max sequence length for tokenization (prompt + answer)\n",
        "NUM_PROC                       = 4        # dataset map/filter workers\n",
        "EVAL_BATCH_SIZE                = 8\n",
        "\n",
        "OUTPUT_DIR                     = \"./outputs\"\n",
        "RUN_NAME                       = \"gemma-2b-mmmlu-de-fr-qlora-corrected-2epochs-lr1e-4\"\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "wandb.init(project=os.environ[\"WANDB_PROJECT\"],\n",
        "           name=RUN_NAME, dir=OUTPUT_DIR, mode=\"online\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "fd5ab582",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "⚠️  HF_TOKEN not set – make sure you accepted Gemma licence via web UI.\n"
          ]
        }
      ],
      "source": [
        "# %%  ── HF login (needed for Gemma) ────────────────────────────────────────────\n",
        "HF_TOKEN = os.getenv(\"HF_TOKEN\")\n",
        "if HF_TOKEN:\n",
        "    login(token=HF_TOKEN, add_to_git_credential=False)\n",
        "else:\n",
        "    print(\"⚠️  HF_TOKEN not set – make sure you accepted Gemma licence via web UI.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "80740a91",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%  ── Tokenizer ────────────────\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, token=HF_TOKEN or True)\n",
        "tokenizer.pad_token    = tokenizer.eos_token # Gemma uses eos_token for padding\n",
        "tokenizer.padding_side = \"left\"   # For Causal LMs, padding on the left is standard for generation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3351912f",
      "metadata": {},
      "source": [
        "## 1. Load, filter & split the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e718d9ac",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 28084 total examples.\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Load DE + FR 'test' splits and merge ───────────────────────────────────\n",
        "raw_ds_parts = [load_dataset(DATASET_NAME, cfg, split=\"test\") for cfg in DATASET_CONFIGS]\n",
        "raw_ds       = concatenate_datasets(raw_ds_parts)\n",
        "print(f\"Loaded {len(raw_ds)} total examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "40c69761",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Filter (num_proc=4): 100%|██████████| 28084/28084 [00:01<00:00, 17819.43 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "After length filter: 23696 examples.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Drop very long questions ────────────────\n",
        "PROMPT_TEMPLATE = \"\"\"The following is a multiple-choice question. \\\n",
        "Provide the letter of the correct answer.\n",
        "\n",
        "Question: {question}\n",
        "Options:\n",
        "(A) {A}\n",
        "(B) {B}\n",
        "(C) {C}\n",
        "(D) {D}\n",
        "Correct Answer:\"\"\"\n",
        "\n",
        "def _prompt_len_ok(ex):\n",
        "    \"\"\"Keep only items whose prompt (excluding the answer part) fits within the token limit.\"\"\"\n",
        "    prompt = PROMPT_TEMPLATE.format(\n",
        "        question = ex[\"Question\"],   # ← use the **uppercase** field names\n",
        "        A = ex[\"A\"], B = ex[\"B\"], C = ex[\"C\"], D = ex[\"D\"]\n",
        "    )\n",
        "    return len(tokenizer(prompt).input_ids) <= MAX_PROMPT_TOKENS_FOR_FILTER\n",
        "\n",
        "ds_filtered = raw_ds.filter(_prompt_len_ok, num_proc=NUM_PROC)\n",
        "print(f\"After length filter: {len(ds_filtered)} examples.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a5cd6f37",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Flattening the indices: 100%|██████████| 23696/23696 [00:00<00:00, 35759.92 examples/s]\n",
            "Casting to class labels: 100%|██████████| 23696/23696 [00:00<00:00, 38400.89 examples/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train 18956 | Val 2370 | Test 2370\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Stratified 80-10-10 split by subject (use built-in label encoding) ─────\n",
        "# 1) Convert the textual “Subject” column into a ClassLabel in one call\n",
        "ds_encoded = ds_filtered.class_encode_column(\"Subject\")   # adds .names mapping\n",
        "\n",
        "# 2) Perform stratified splits on this encoded column\n",
        "train_val_test = ds_encoded.train_test_split(\n",
        "    test_size=0.20, seed=42, stratify_by_column=\"Subject\"\n",
        ")\n",
        "val_test_split = train_val_test[\"test\"].train_test_split(\n",
        "    test_size=0.50, seed=42, stratify_by_column=\"Subject\"\n",
        ")\n",
        "\n",
        "raw_train = train_val_test[\"train\"]\n",
        "raw_val   = val_test_split[\"train\"]\n",
        "raw_test  = val_test_split[\"test\"]\n",
        "\n",
        "print(f\"Train {len(raw_train)} | Val {len(raw_val)} | Test {len(raw_test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c595db6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map (num_proc=4): 100%|██████████| 18956/18956 [00:03<00:00, 5009.14 examples/s] \n",
            "Map (num_proc=4): 100%|██████████| 2370/2370 [00:00<00:00, 7772.11 examples/s]\n",
            "Map (num_proc=4): 100%|██████████| 2370/2370 [00:00<00:00, 7976.85 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Build tokenised training / validation sets ────────────────────────────\n",
        "def _tokenise(example):\n",
        "    prompt_text = PROMPT_TEMPLATE.format(**{\n",
        "        \"question\": example[\"Question\"],\n",
        "        \"A\": example[\"A\"], \"B\": example[\"B\"],\n",
        "        \"C\": example[\"C\"], \"D\": example[\"D\"]\n",
        "    })\n",
        "    # Tokenize the prompt part first\n",
        "    prompt_tokenized = tokenizer(prompt_text, add_special_tokens=False) # No BOS/EOS for prompt part alone\n",
        "    target_letter = example[\"Answer\"].strip().upper()\n",
        "    target_text_with_space = \" \" + target_letter \n",
        "    target_tokenized = tokenizer(target_text_with_space, add_special_tokens=False)\n",
        "\n",
        "    # Combine prompt and target tokens for input_ids\n",
        "    # Add BOS at the beginning and EOS at the end of the combined sequence\n",
        "    input_ids = [tokenizer.bos_token_id] + prompt_tokenized['input_ids'] + target_tokenized['input_ids'] + [tokenizer.eos_token_id]\n",
        "    attention_mask = [1] * len(input_ids)\n",
        "\n",
        "    # Create labels: mask prompt tokens, keep answer tokens and the final EOS\n",
        "    labels = [-100] * (1 + len(prompt_tokenized['input_ids'])) + target_tokenized['input_ids'] + [tokenizer.eos_token_id]\n",
        "\n",
        "    # Pad sequences to MAX_SEQ_LENGTH\n",
        "    padding_length = MAX_SEQ_LENGTH - len(input_ids)\n",
        "    if padding_length > 0:\n",
        "        # Pad on the left for Causal LMs\n",
        "        input_ids = [tokenizer.pad_token_id] * padding_length + input_ids\n",
        "        attention_mask = [0] * padding_length + attention_mask\n",
        "        labels = [-100] * padding_length + labels\n",
        "    elif padding_length < 0:\n",
        "        # Truncate from the right \n",
        "        input_ids = input_ids[:MAX_SEQ_LENGTH]\n",
        "        attention_mask = attention_mask[:MAX_SEQ_LENGTH]\n",
        "        labels = labels[:MAX_SEQ_LENGTH]\n",
        "        # Ensure the last token is EOS if truncated, and its label is EOS or -100 if it was pad before\n",
        "        if input_ids[-1] != tokenizer.eos_token_id:\n",
        "             input_ids[-1] = tokenizer.eos_token_id\n",
        "             if labels[-1] != -100 : # Only change if it was a valid label before truncation made it not EOS\n",
        "                labels[-1] = tokenizer.eos_token_id # Or -100 if we don't want to predict EOS after truncation\n",
        "    \n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"labels\": labels,\n",
        "        \"label_letter\": target_letter, # Keep original letter for accuracy calc\n",
        "        \"subject\": example[\"Subject\"]\n",
        "    }\n",
        "\n",
        "train_ds = raw_train.map(_tokenise, remove_columns=raw_train.column_names, num_proc=NUM_PROC)\n",
        "val_ds   = raw_val.map(_tokenise, remove_columns=raw_val.column_names, num_proc=NUM_PROC)\n",
        "test_ds  = raw_test.map(_tokenise, remove_columns=raw_test.column_names, num_proc=NUM_PROC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "465b8dec",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:22<00:00,  7.46s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 41,533,440 || all params: 2,655,875,328 || trainable%: 1.5638\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Load Gemma-2-2 B with 4-bit QLoRA (eager attn, cache off) ─────────────\n",
        "bnb_cfg = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "base_model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    quantization_config=bnb_cfg,\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\",   # ← recommended for Gemma-2\n",
        "    token=HF_TOKEN or True,\n",
        ")\n",
        "\n",
        "# Disable KV-cache during training (needed when checkpointing is on)\n",
        "base_model.config.use_cache = False\n",
        "base_model.gradient_checkpointing_enable()\n",
        "\n",
        "base_model = prepare_model_for_kbit_training(base_model)\n",
        "\n",
        "lora_cfg = LoraConfig(\n",
        "    r=32, lora_alpha=16, lora_dropout=0.05,\n",
        "    task_type=\"CAUSAL_LM\", bias=\"none\",\n",
        "    # Gemma's linear layers are typically 'o_proj', 'k_proj', 'q_proj', 'v_proj', 'gate_proj', 'up_proj', 'down_proj'\n",
        "    # Targeting all linear layers for LoRA adaptation is a common strategy.\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"] \n",
        ")\n",
        "model = get_peft_model(base_model, lora_cfg)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea3d5da2",
      "metadata": {},
      "source": [
        "## 2. Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc3e7c48",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Trainer setup ──\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir           = OUTPUT_DIR,\n",
        "    run_name             = RUN_NAME,\n",
        "    bf16                 = torch.cuda.is_available(), # Use bfloat16 if available\n",
        "    per_device_train_batch_size = 4,\n",
        "    per_device_eval_batch_size  = 4,\n",
        "    gradient_accumulation_steps = 4, # Effective batch size = 4*4 = 16\n",
        "    num_train_epochs     = 2, # Increased epochs\n",
        "    learning_rate        = 1e-4, # Reduced learning rate\n",
        "    lr_scheduler_type    = \"cosine\", # Added scheduler\n",
        "    warmup_ratio         = 0.05,    # Added warmup\n",
        "    logging_steps        = 25,\n",
        "    eval_strategy        = \"epoch\",   \n",
        "    save_strategy        = \"epoch\",\n",
        "    save_total_limit     = 1, # Save only the best model\n",
        "    load_best_model_at_end = True, # Load the best model at the end of training\n",
        "    report_to            = [\"wandb\"],\n",
        "    seed                 = 42,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model           = model,\n",
        "    args            = training_args,\n",
        "    train_dataset   = train_ds,\n",
        "    eval_dataset    = val_ds,\n",
        "    data_collator   = default_data_collator, # default_data_collator handles labels correctly if present\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfaf224f",
      "metadata": {},
      "source": [
        "## 3. Baseline accuracy (model **before** finetune)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3adf5656",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%  ── Helper: extract first answer letter ─────\n",
        "import re\n",
        "LETTER_RE = re.compile(r\"\\b([A-Da-d])\\b\")\n",
        "\n",
        "def _first_letter(text: str) -> str:\n",
        "    \"\"\"Return the first occurrence of A-D (case-insensitive) or '' if absent.\"\"\"\n",
        "    match = LETTER_RE.search(text)\n",
        "    return match.group(1).upper() if match else \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fca815c1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# %%  ── Helper: accuracy evaluator ───────────────────\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "def accuracy_on_raw(raw_dataset_eval, desc=\"Baseline eval\"):\n",
        "    \"\"\"Generate answers batch-wise, show progress, and compute accuracy.\"\"\"\n",
        "    model.eval() # Ensure model is in evaluation mode\n",
        "    device   = next(model.parameters()).device\n",
        "    correct  = total = 0\n",
        "    n_batches = (len(raw_dataset_eval) + EVAL_BATCH_SIZE - 1) // EVAL_BATCH_SIZE\n",
        "\n",
        "    for idx in tqdm(range(n_batches), desc=desc):\n",
        "        start = idx * EVAL_BATCH_SIZE\n",
        "        end   = min(start + EVAL_BATCH_SIZE, len(raw_dataset_eval))\n",
        "        batch = raw_dataset_eval.select(range(start, end))\n",
        "\n",
        "        prompts = [\n",
        "            PROMPT_TEMPLATE.format(\n",
        "                question = q,\n",
        "                A = a, B = b, C = c, D = d\n",
        "            )\n",
        "            for q, a, b, c, d in zip(\n",
        "                batch[\"Question\"], batch[\"A\"], batch[\"B\"],\n",
        "                batch[\"C\"], batch[\"D\"]\n",
        "            )\n",
        "        ]\n",
        "        \n",
        "        # Tokenize prompts without adding bos/eos here, as generate will handle it\n",
        "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_PROMPT_TOKENS_FOR_FILTER).to(device)\n",
        "\n",
        "        with torch.no_grad(), torch.autocast(device.type if device.type != 'mps' else 'cpu', dtype=torch.bfloat16, enabled=(device.type==\"cuda\")):\n",
        "            # Ensure generation starts after the prompt by using input_ids as prefix\n",
        "            gen_ids = model.generate(\n",
        "                input_ids=inputs[\"input_ids\"],\n",
        "                attention_mask=inputs[\"attention_mask\"],\n",
        "                max_new_tokens=5, # Enough for \" A\" + EOS or similar\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        for ans, g_full in zip(batch[\"Answer\"], gen_ids):\n",
        "            # Decode only the generated part (after the prompt)\n",
        "            prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "            generated_tokens = g_full[prompt_len:]\n",
        "            \n",
        "            pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "            pred_letter = _first_letter(pred_text) # Extract first A-D letter\n",
        "            \n",
        "            correct += int(pred_letter == ans.strip().upper())\n",
        "            total   += 1\n",
        "\n",
        "    return correct / total if total else 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "31de1fd6",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Baseline test eval: 100%|██████████| 297/297 [07:21<00:00,  1.49s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Baseline (pre-FT) TEST accuracy: 0.3751\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Baseline inference accuracy on TEST set ────────────────\n",
        "baseline_test_acc = accuracy_on_raw(raw_test, desc=\"Baseline test eval\") # Evaluate on raw_test\n",
        "print(f\"🔹 Baseline (pre-FT) TEST accuracy: {baseline_test_acc:.4f}\")\n",
        "wandb.log({\"baseline_test_accuracy\": baseline_test_acc})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "2685e84f",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2370' max='2370' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2370/2370 3:44:45, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.547700</td>\n",
              "      <td>0.522862</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.338800</td>\n",
              "      <td>0.538910</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=2370, training_loss=0.4726693630218506, metrics={'train_runtime': 13491.651, 'train_samples_per_second': 2.81, 'train_steps_per_second': 0.176, 'total_flos': 1.409906483048448e+17, 'train_loss': 0.4726693630218506, 'epoch': 2.0})"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# %%  ── Finetune ───────────────────────────────────────────────────────────────\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a690521",
      "metadata": {},
      "source": [
        "## 4. Final Evaluation and Inference (Example)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "5fd6676b",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1186' max='593' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [593/593 08:09]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Post-FT validation ppl: 1.69\n",
            "🔹 Post-FT test       ppl: 1.69\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Perplexity on validation / test sets ───────────────────────────────────\n",
        "val_metrics  = trainer.evaluate(eval_dataset=val_ds) # Uses tokenized val_ds\n",
        "test_metrics = trainer.evaluate(eval_dataset=test_ds) # Uses tokenized test_ds\n",
        "\n",
        "val_ppl  = math.exp(val_metrics[\"eval_loss\"])\n",
        "test_ppl = math.exp(test_metrics[\"eval_loss\"])\n",
        "\n",
        "print(f\"🔹 Post-FT validation ppl: {val_ppl:.2f}\")\n",
        "print(f\"🔹 Post-FT test       ppl: {test_ppl:.2f}\")\n",
        "\n",
        "wandb.log({\"final_val_loss\": val_metrics[\"eval_loss\"],\n",
        "           \"final_val_ppl\" : val_ppl,\n",
        "           \"final_test_loss\": test_metrics[\"eval_loss\"],\n",
        "           \"final_test_ppl\" : test_ppl})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "8281a8f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Post-FT test eval: 100%|██████████| 297/297 [03:21<00:00,  1.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🔹 Post-FT TEST accuracy: 0.5392\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Accuracy after finetune on TEST set ───────────────────────────────────\n",
        "ft_test_accuracy = accuracy_on_raw(raw_test, desc=\"Post-FT test eval\") # Evaluate on raw_test\n",
        "print(f\"🔹 Post-FT TEST accuracy: {ft_test_accuracy:.4f}\")\n",
        "wandb.log({\"post_ft_test_accuracy\": ft_test_accuracy})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "53edbf9f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Question: Was ist die Hauptstadt von Frankreich?\n",
            "Options:\n",
            "  (A) Berlin\n",
            "  (B) Paris\n",
            "  (C) London\n",
            "  (D) Madrid\n",
            "Model answer letter: (B)\n"
          ]
        }
      ],
      "source": [
        "# %%  ── Example inference ─────────────────────────────\n",
        "example_q     = \"Was ist die Hauptstadt von Frankreich?\"\n",
        "example_opts  = {\"A\":\"Berlin\",\"B\":\"Paris\",\"C\":\"London\",\"D\":\"Madrid\"}\n",
        "\n",
        "example_prompt = PROMPT_TEMPLATE.format(\n",
        "    question = example_q,\n",
        "    A = example_opts[\"A\"],\n",
        "    B = example_opts[\"B\"],\n",
        "    C = example_opts[\"C\"],\n",
        "    D = example_opts[\"D\"]\n",
        ")\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # Ensure model is on the correct device\n",
        "model.eval() # Ensure model is in eval mode for inference\n",
        "\n",
        "inputs = tokenizer(example_prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_PROMPT_TOKENS_FOR_FILTER).to(device)\n",
        "\n",
        "with torch.no_grad(), torch.autocast(device.type if device.type != 'mps' else 'cpu', dtype=torch.bfloat16, enabled=(device.type==\"cuda\")):\n",
        "    gen_ids = model.generate(\n",
        "        input_ids=inputs[\"input_ids\"],\n",
        "        attention_mask=inputs[\"attention_mask\"],\n",
        "        max_new_tokens=5, \n",
        "        do_sample=False,\n",
        "        pad_token_id=tokenizer.eos_token_id\n",
        "    )\n",
        "\n",
        "prompt_len = inputs[\"input_ids\"].shape[1]\n",
        "generated_tokens = gen_ids[0][prompt_len:] # Get only generated tokens\n",
        "generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "generated_letter = _first_letter(generated_text)\n",
        "\n",
        "print(f\"\\nQuestion: {example_q}\")\n",
        "print(\"Options:\")\n",
        "for k,v in example_opts.items():\n",
        "    print(f\"  ({k}) {v}\")\n",
        "print(f\"Model answer letter: ({generated_letter})\")\n",
        "wandb.log({\"example_question\": example_q, \"example_answer_predicted\": generated_letter})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "wandb_finish_cell",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>baseline_test_accuracy</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁█▁▁</td></tr><tr><td>eval/runtime</td><td>█▅▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▄▄█</td></tr><tr><td>eval/steps_per_second</td><td>▁▅▄█</td></tr><tr><td>final_test_loss</td><td>▁</td></tr><tr><td>final_test_ppl</td><td>▁</td></tr><tr><td>final_val_loss</td><td>▁</td></tr><tr><td>final_val_ppl</td><td>▁</td></tr><tr><td>post_ft_test_accuracy</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇▇██████</td></tr><tr><td>train/grad_norm</td><td>▃▄▄▁▂▂▂▃▂▇▃▇▂▅▃▃▃▃▆▄▃█▅▅▁▃▄▂▇▄▄▅▄▄▄▇▂▄▅▂</td></tr><tr><td>train/learning_rate</td><td>▂▄▅███████▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▃▃▃▃▃▃▃▃▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▂▁▁▂▁▁▁▁▁▁▁▁▁▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>baseline_test_accuracy</td><td>0.37511</td></tr><tr><td>eval/loss</td><td>0.52199</td></tr><tr><td>eval/runtime</td><td>244.3603</td></tr><tr><td>eval/samples_per_second</td><td>9.699</td></tr><tr><td>eval/steps_per_second</td><td>2.427</td></tr><tr><td>example_answer_predicted</td><td>B</td></tr><tr><td>example_question</td><td>Was ist die Hauptsta...</td></tr><tr><td>final_test_loss</td><td>0.52199</td></tr><tr><td>final_test_ppl</td><td>1.68537</td></tr><tr><td>final_val_loss</td><td>0.52286</td></tr><tr><td>final_val_ppl</td><td>1.68685</td></tr><tr><td>post_ft_test_accuracy</td><td>0.53924</td></tr><tr><td>total_flos</td><td>1.409906483048448e+17</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>2370</td></tr><tr><td>train/grad_norm</td><td>2.24715</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.3388</td></tr><tr><td>train_loss</td><td>0.47267</td></tr><tr><td>train_runtime</td><td>13491.651</td></tr><tr><td>train_samples_per_second</td><td>2.81</td></tr><tr><td>train_steps_per_second</td><td>0.176</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">gemma-2b-mmmlu-de-fr-qlora-corrected-2epochs-lr1e-4</strong> at: <a href='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected/runs/inhq9y7f' target=\"_blank\">https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected/runs/inhq9y7f</a><br> View project at: <a href='https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected' target=\"_blank\">https://wandb.ai/denis-katkalo-kpi/gemma-nlp_lab2_mmmlu_ft_corrected</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./outputs/wandb/run-20250528_015916-inhq9y7f/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
